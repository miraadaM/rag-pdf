ğŸš€ RAG PDF + Ollama Pipeline

This project implements a **lightweight Retrieval-Augmented Generation (RAG) pipeline** that lets you query PDFs using **FAISS vector search** and **Ollama LLMs** (e.g., `phi3`).  

Itâ€™s perfect for creating your own **local AI knowledge base** from research papers, reports, or textbooks.

---

## âœ¨ Features
- ğŸ“„ **PDF Extraction** with [PyMuPDF](https://pymupdf.readthedocs.io/)
- âœ‚ï¸ **Smart Chunking** by custom markers (`$$$`) and token size
- ğŸ§© **Embeddings** via [Ollama Embeddings (`nomic-embed-text`)](https://ollama.ai/)
- ğŸ” **Semantic Search** with [FAISS](https://github.com/facebookresearch/faiss)
- ğŸ¤– **Context-Aware Q&A** powered by local Ollama models (tested with `phi3`)
- ğŸ—‚ï¸ **Inspect FAISS Database** (metadata + vectors)

  ## âš¡ Quickstart

### 1. Clone Repo
```bash
git clone https://github.com/YOUR_USERNAME/rag-pdf-ollama.git
cd rag-pdf-ollama
Install dependencies: pip install -r requirements.txt
Run the pipeline and plase your pdf to repo: python chunking.py
The program will:
Extract text from the PDF
Split into chunks
Build FAISS index
Allow interactive Q&A via terminal
ğŸ”§ Configuration
marker="$$$" â†’ split by custom markers inside PDF text
max_tokens=500 â†’ max token length per chunk
Default embedding model â†’ nomic-embed-text
Default LLM â†’ phi3
